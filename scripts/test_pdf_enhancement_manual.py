"""æ‰‹åŠ¨æµ‹è¯•PDF EnhancementåŠŸèƒ½"""

import asyncio
import sys
from datetime import datetime
from pathlib import Path

sys.path.append(str(Path(__file__).resolve().parent.parent))

from src.enhancer import PDFEnhancer
from src.models import RawCandidate
from src.scorer import LLMScorer


async def test_single_arxiv_paper():
    """æµ‹è¯•å•ç¯‡arXivè®ºæ–‡çš„PDF EnhancementåŠŸèƒ½"""

    # ä½¿ç”¨å·²çŸ¥çš„MGXç›¸å…³è®ºæ–‡ï¼ˆSWE-benchï¼‰
    test_paper = RawCandidate(
        title="SWE-bench: Can Language Models Resolve Real-World GitHub Issues?",
        url="https://arxiv.org/abs/2310.06770",
        source="arxiv",
        abstract="We introduce SWE-bench, an evaluation framework consisting of 2,294 software engineering problems drawn from real GitHub issues and corresponding pull requests across 12 popular Python repositories.",
        paper_url="https://arxiv.org/abs/2310.06770",
        publish_date=datetime(2023, 10, 10),
    )

    print("=" * 60)
    print("æµ‹è¯•ï¼šPDF EnhancementåŠŸèƒ½éªŒè¯")
    print("=" * 60)
    print(f"\næµ‹è¯•è®ºæ–‡: {test_paper.title}")
    print(f"arXiv URL: {test_paper.paper_url}")

    # Step 1: PDF Enhancement
    print("\n" + "-" * 60)
    print("Step 1: PDF Enhancementï¼ˆæå–6ä¸ªç« èŠ‚ï¼‰")
    print("-" * 60)

    enhancer = PDFEnhancer()
    try:
        enhanced = await enhancer.enhance_candidate(test_paper)

        print("\nğŸ“„ PDF Enhancementç»“æœ:")
        print(f"  - åŸå§‹æ‘˜è¦é•¿åº¦: {len(test_paper.abstract or '')} chars")

        if enhanced.raw_metadata:
            print(f"  - raw_metadata keys: {list(enhanced.raw_metadata.keys())}")

            # æå–6ä¸ªç« èŠ‚æ‘˜è¦
            introduction = enhanced.raw_metadata.get('introduction_summary', '')
            method = enhanced.raw_metadata.get('method_summary', '')
            evaluation = enhanced.raw_metadata.get('evaluation_summary', '')
            dataset = enhanced.raw_metadata.get('dataset_summary', '')
            baselines = enhanced.raw_metadata.get('baselines_summary', '')
            conclusion = enhanced.raw_metadata.get('conclusion_summary', '')

            print(f"\n  ç« èŠ‚æå–ç»“æœ:")
            print(f"    - introduction_summary: {len(introduction)} chars")
            print(f"    - method_summary: {len(method)} chars")
            print(f"    - evaluation_summary: {len(evaluation)} chars")
            print(f"    - dataset_summary: {len(dataset)} chars")
            print(f"    - baselines_summary: {len(baselines)} chars")
            print(f"    - conclusion_summary: {len(conclusion)} chars")

            total_pdf_content = sum([
                len(introduction),
                len(method),
                len(evaluation),
                len(dataset),
                len(baselines),
                len(conclusion),
            ])
            print(f"\n  - PDFæ€»å†…å®¹é•¿åº¦: {total_pdf_content} chars")

            # éªŒè¯P1æ ¸å¿ƒç« èŠ‚æ•°é‡
            p1_sections = [introduction, method, evaluation, dataset]
            p1_count = sum(1 for s in p1_sections if s and not s.startswith("æœªæä¾›"))
            print(f"  - P1æ ¸å¿ƒç« èŠ‚æ•°é‡: {p1_count}/4")

            # éªŒè¯P2è¾…åŠ©ç« èŠ‚æ•°é‡
            p2_sections = [baselines, conclusion]
            p2_count = sum(1 for s in p2_sections if s and not s.startswith("æœªæä¾›"))
            print(f"  - P2è¾…åŠ©ç« èŠ‚æ•°é‡: {p2_count}/2")

            # è´¨é‡è¯„ä¼°
            if p1_count >= 2:
                print(f"\n  âœ… PDF Enhancementè´¨é‡è¾¾æ ‡ï¼ˆP1â‰¥2ï¼‰")
            else:
                print(f"\n  âš ï¸ PDF Enhancementè´¨é‡ä¸è¶³ï¼ˆP1<2ï¼‰")

        else:
            print("  âŒ raw_metadataä¸ºç©ºï¼ŒPDF Enhancementå¤±è´¥")
            return

    except Exception as e:
        print(f"\n  âŒ PDF Enhancementå¤±è´¥: {e}")
        return

    # Step 2: LLMè¯„åˆ†
    print("\n" + "-" * 60)
    print("Step 2: LLMè¯„åˆ†ï¼ˆéªŒè¯æ¨ç†é•¿åº¦æå‡ï¼‰")
    print("-" * 60)

    async with LLMScorer() as scorer:
        try:
            scored = await scorer.score(enhanced)

            print("\nğŸ¯ LLMè¯„åˆ†ç»“æœ:")

            # ç»Ÿè®¡æ¨ç†å­—æ®µé•¿åº¦
            reasoning_fields = {
                'activity_reasoning': scored.activity_reasoning,
                'reproducibility_reasoning': scored.reproducibility_reasoning,
                'license_reasoning': scored.license_reasoning,
                'novelty_reasoning': scored.novelty_reasoning,
                'relevance_reasoning': scored.relevance_reasoning,
                'overall_reasoning': scored.overall_reasoning,
            }

            for field_name, reasoning_text in reasoning_fields.items():
                print(f"  - {field_name}: {len(reasoning_text)} chars")

            # è®¡ç®—æ€»æ¨ç†é•¿åº¦
            total_reasoning = sum(len(text) for text in reasoning_fields.values())
            print(f"\n  - æ¨ç†æ€»å­—æ•°: {total_reasoning} chars")

            # éªŒè¯æ˜¯å¦è¾¾æ ‡
            if total_reasoning >= 1200:
                print(f"  âœ… æ¨ç†æ€»å­—æ•°è¾¾æ ‡ï¼ˆâ‰¥1200ï¼‰")
            else:
                print(f"  âŒ æ¨ç†æ€»å­—æ•°ä¸è¶³ï¼ˆ{total_reasoning} < 1200ï¼‰")

            # æ˜¾ç¤ºè¯„åˆ†
            print(f"\n  è¯„åˆ†ç»“æœ:")
            print(f"    - activity_score: {scored.activity_score}/10")
            print(f"    - reproducibility_score: {scored.reproducibility_score}/10")
            print(f"    - license_score: {scored.license_score}/10")
            print(f"    - novelty_score: {scored.novelty_score}/10")
            print(f"    - relevance_score: {scored.relevance_score}/10")

        except Exception as e:
            print(f"\n  âŒ LLMè¯„åˆ†å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return

    print("\n" + "=" * 60)
    print("âœ… æµ‹è¯•å®Œæˆï¼")
    print("=" * 60)


if __name__ == "__main__":
    asyncio.run(test_single_arxiv_paper())
