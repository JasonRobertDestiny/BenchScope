# BenchScope æ•°æ®æå–åŠŸèƒ½å®ŒæˆçŠ¶æ€æŠ¥å‘Š

**ç”Ÿæˆæ—¶é—´**: 2025-11-17
**ç‰ˆæœ¬**: Phase 7 åç«¯æ‰©å±•ç‰ˆ

---

## âœ… å·²å®Œæˆçš„æ•°æ®æå–åŠŸèƒ½

### 1. GitHubé‡‡é›†å™¨ - READMEæå– âœ…

**æ–‡ä»¶**: `src/collectors/github_collector.py`

**æå–æ–¹æ³•**: `_extract_raw_metadata(readme_text)` (ç¬¬261-309è¡Œ)

#### æå–çš„å­—æ®µï¼š

| å­—æ®µ | æå–æ–¹å¼ | ç¤ºä¾‹ | ä»£ç ä½ç½® |
|------|---------|------|---------|
| **metrics** | æ­£åˆ™åŒ¹é…11ç§å¸¸è§æŒ‡æ ‡ | Pass@1, BLEU-4, ROUGE-L, F1-Score, Accuracy | 270-292è¡Œ |
| **baselines** | æ­£åˆ™åŒ¹é…9ç§ä¸»æµæ¨¡å‹ | GPT-4, Claude-3.5, Llama-3.1, StarCoder | 294-301è¡Œ |
| **dataset_size** | æ­£åˆ™åŒ¹é…æ•°æ®è§„æ¨¡ | "1000 problems", "10K samples" | 303-308è¡Œ |
| **dataset_url** | URLExtractoræ™ºèƒ½æå– | HuggingFace/GitHubæ•°æ®é›†é“¾æ¥ | 176è¡Œ |
| **task_type** | å…³é”®è¯åŒ¹é…ä»»åŠ¡ç±»å‹ | code generation, web automation | 171è¡Œ |

#### æ­£åˆ™è¡¨è¾¾å¼è§„åˆ™ï¼š

**è¯„ä¼°æŒ‡æ ‡** (11ç§):
```python
METRIC_PATTERNS = {
    r"pass@\d+": "PASS",                    # Pass@1, Pass@10
    r"bleu(?:-\d+)?": "BLEU",              # BLEU, BLEU-4
    r"rouge(?:-[l1-3])?": "ROUGE",         # ROUGE-L, ROUGE-1
    r"f1-?score": "F1-Score",
    r"accuracy": "Accuracy",
    r"precision": "Precision",
    r"recall": "Recall",
    r"exact match": "Exact Match",
    r"code pass rate": "Code Pass Rate",
    r"success rate": "Success Rate",
}
```

**Baselineæ¨¡å‹** (9ç§):
```python
BASELINE_PATTERNS = {
    r"gpt-4(?:-turbo|-o)?": "GPT-4",
    r"gpt-3\.5(?:-turbo)?": "GPT-3.5",
    r"claude[\s-]?(?:3\.5|3|opus|sonnet)": "Claude",
    r"llama[-\s]?3(?:\.1)?-?\d{1,3}[mb]?": "Llama",
    r"code\s?llama": "Code Llama",
    r"starcoder": "StarCoder",
    r"codex": "Codex",
    r"mistral": "Mistral",
    r"deepseek": "DeepSeek",
}
```

**æ•°æ®é›†è§„æ¨¡** (2ç§æ¨¡å¼):
```python
DATASET_SIZE_PATTERNS = [
    r"\b\d{1,3}(?:[,\s]\d{3})*(?:\s*(?:k|m))?\s*(?:samples?|problems?|questions?|tasks?|examples?|test\s+cases?)\b",
    r"(?:contains|includes|consists\s+of)\s+\d{1,3}(?:[,\s]\d{3})*(?:\s*(?:k|m))?\s*\w*",
]
```

#### æ•°æ®æµå‘ï¼š

```
GitHub README
    â†“ (æ­£åˆ™æå–)
ReadmeExtraction {metrics, baselines, dataset_size}
    â†“
RawCandidate {
    raw_metrics: ["Pass@1", "Accuracy"],
    raw_baselines: ["GPT-4", "Claude"],
    raw_dataset_size: "1000 problems"
}
    â†“
LLMè¯„åˆ†å™¨ (ä½œä¸ºå‚è€ƒè¾“å…¥)
    â†“
ScoredCandidate {
    metrics: ["Pass@1", "Accuracy"],      # LLMæ¸…æ´—å
    baselines: ["GPT-4", "Claude-3.5"],   # LLMè§„èŒƒåŒ–
    dataset_size: 1000,                   # LLMè§£æä¸ºæ•°å­—
    dataset_size_description: "1000 problems"
}
```

---

### 2. arXivé‡‡é›†å™¨ - æ‘˜è¦æå– âœ…

**æ–‡ä»¶**: `src/collectors/arxiv_collector.py`

**æå–æ–¹æ³•**: `_to_candidates()` (ç¬¬72-117è¡Œ)

#### æå–çš„å­—æ®µï¼š

| å­—æ®µ | æå–æ–¹å¼ | ç¤ºä¾‹ | ä»£ç ä½ç½® |
|------|---------|------|---------|
| **paper_url** | arXiv APIç›´æ¥è¿”å› | https://arxiv.org/abs/2401.12345 | 104è¡Œ |
| **dataset_url** | URLExtractorä»æ‘˜è¦æå– | HuggingFace/GitHubé“¾æ¥ | 94è¡Œ |
| **authors** | arXiv APIè¿”å› | ["Alice Zhang", "Bob Li"] | 102è¡Œ |
| **raw_authors** | å­—ç¬¦ä¸²æ‹¼æ¥ | "Alice Zhang, Bob Li" | 106è¡Œ |
| **raw_institutions** | ä»ä½œè€…affiliationæå– | "Stanford, MIT" | 107è¡Œ |
| **abstract** | arXiv APIè¿”å› | å®Œæ•´è®ºæ–‡æ‘˜è¦ | 101è¡Œ |

#### æ•°æ®æµå‘ï¼š

```
arXiv è®ºæ–‡æ‘˜è¦
    â†“ (APIè¿”å›)
arxiv.Result {summary, authors, entry_id}
    â†“ (URLExtractoræå–)
RawCandidate {
    paper_url: "https://arxiv.org/abs/2401.12345",
    dataset_url: "https://huggingface.co/datasets/xxx",
    abstract: "å®Œæ•´æ‘˜è¦...",
    raw_authors: "Alice Zhang, Bob Li",
    raw_institutions: "Stanford, MIT"
}
    â†“
LLMè¯„åˆ†å™¨ (ä»æ‘˜è¦ä¸­æ™ºèƒ½æå–)
    â†“
ScoredCandidate {
    metrics: ["Pass@1", "BLEU-4"],        # LLMä»æ‘˜è¦æå–
    baselines: ["GPT-4", "Claude"],       # LLMä»æ‘˜è¦æå–
    dataset_size: 1000,                   # LLMä»æ‘˜è¦è§£æ
    institution: "Stanford University",   # LLMè§„èŒƒåŒ–
    authors: ["Alice Zhang", "Bob Li"]    # LLMæ¸…æ´—
}
```

---

### 3. LLMè¯„åˆ†å™¨ - æ™ºèƒ½æŠ½å– âœ…

**æ–‡ä»¶**: `src/scorer/llm_scorer.py`

**è¯„åˆ†æ¨¡å‹**: GPT-4o (50å¹¶å‘)

#### LLMæŠ½å–çš„å­—æ®µï¼š

| å­—æ®µ | è¾“å…¥ | è¾“å‡º | è¯´æ˜ |
|------|------|------|------|
| **task_domain** | æ‘˜è¦/READMEå…¨æ–‡ | "Coding,Backend" | ä»10ä¸ªé€‰é¡¹ä¸­é€‰æ‹©ï¼ˆå¤šé€‰ï¼‰ |
| **metrics** | raw_metrics + æ‘˜è¦ | ["Pass@1", "BLEU-4"] | è§„èŒƒåŒ–æ ¼å¼ï¼Œæœ€å¤š5ä¸ª |
| **baselines** | raw_baselines + æ‘˜è¦ | ["GPT-4", "Claude-3.5"] | è§„èŒƒåŒ–æ ¼å¼ï¼Œæœ€å¤š5ä¸ª |
| **institution** | raw_institutions + æ‘˜è¦ | "Stanford University" | æå–ä¸»è¦æœºæ„ |
| **authors** | raw_authors + æ‘˜è¦ | ["Alice Zhang", "Bob Li"] | æœ€å¤š5äºº |
| **dataset_size** | raw_dataset_size + æ‘˜è¦ | 1000 | è§£æä¸ºæ•´æ•° |
| **dataset_size_description** | raw_dataset_size + æ‘˜è¦ | "1000 coding problems" | ä¿ç•™åŸå§‹æè¿° |

#### Promptç­–ç•¥ (ç¬¬25-78è¡Œ):

```
ã€å€™é€‰ä¿¡æ¯ã€‘
- æ ‡é¢˜: {title}
- æ‘˜è¦/README(æˆªæ–­): {abstract}
- åŸå§‹æŒ‡æ ‡: {raw_metrics}           â† GitHubè§„åˆ™æå–
- åŸå§‹Baseline: {raw_baselines}     â† GitHubè§„åˆ™æå–
- åŸå§‹ä½œè€…: {raw_authors}           â† arXiv APIæå–
- åŸå§‹æœºæ„: {raw_institutions}      â† arXiv APIæå–
- åŸå§‹æ•°æ®è§„æ¨¡: {raw_dataset_size}  â† GitHubè§„åˆ™æå–
```

**LLMå·¥ä½œæ¨¡å¼**:
1. **ä¼˜å…ˆä½¿ç”¨åŸå§‹æå–æ•°æ®** (raw_*)
2. **è¡¥å……ç¼ºå¤±ä¿¡æ¯** (ä»æ‘˜è¦/READMEå…¨æ–‡åˆ†æ)
3. **è§„èŒƒåŒ–æ ¼å¼** (å¦‚ "gpt4" â†’ "GPT-4")
4. **æ™ºèƒ½è§£æ** (å¦‚ "1K samples" â†’ 1000)

---

## ğŸ”„ æ•°æ®æå–å®Œæ•´æµç¨‹

### GitHub â†’ LLM æµç¨‹ï¼š

```
1. GitHub Search API
   â†“
2. _fetch_readme() - è·å–READMEåŸæ–‡
   â†“
3. _extract_raw_metadata() - æ­£åˆ™æå–
   raw_metrics: ["Pass@1", "accuracy"]
   raw_baselines: ["gpt-4", "claude"]
   raw_dataset_size: "1000 problems"
   â†“
4. RawCandidate (å­˜å‚¨åŸå§‹æ•°æ®)
   â†“
5. LLMè¯„åˆ†å™¨ (æ™ºèƒ½æ¸…æ´—)
   - è§„èŒƒåŒ–: "gpt-4" â†’ "GPT-4"
   - å»é‡: ["accuracy", "Accuracy"] â†’ ["Accuracy"]
   - è§£æ: "1000 problems" â†’ dataset_size=1000
   â†“
6. ScoredCandidate (æœ€ç»ˆæ•°æ®)
   metrics: ["Pass@1", "Accuracy"]
   baselines: ["GPT-4", "Claude-3.5"]
   dataset_size: 1000
```

### arXiv â†’ LLM æµç¨‹ï¼š

```
1. arXiv API
   â†“
2. è®ºæ–‡æ‘˜è¦ + å…ƒæ•°æ®
   abstract: "We introduce HumanEval..."
   authors: [arxiv.Author objects]
   â†“
3. _extract_authors_institutions() - ç»“æ„åŒ–
   raw_authors: "Alice Zhang, Bob Li"
   raw_institutions: "Stanford, MIT"
   â†“
4. URLExtractor.extract_dataset_url() - URLæå–
   dataset_url: "https://huggingface.co/datasets/xxx"
   â†“
5. RawCandidate (å­˜å‚¨åŸå§‹æ•°æ®)
   â†“
6. LLMè¯„åˆ†å™¨ (ä»æ‘˜è¦æ™ºèƒ½æå–)
   - è¯»å–æ‘˜è¦å…¨æ–‡
   - è¯†åˆ«è¯„ä¼°æŒ‡æ ‡: "We evaluate using Pass@1"
   - è¯†åˆ«Baseline: "compared to GPT-4"
   - è§£ææ•°æ®è§„æ¨¡: "164 hand-written problems"
   â†“
7. ScoredCandidate (æœ€ç»ˆæ•°æ®)
   metrics: ["Pass@1"]
   baselines: ["GPT-4"]
   dataset_size: 164
   dataset_size_description: "164 hand-written problems"
```

---

## ğŸ“Š æ•°æ®è¦†ç›–ç‡ä¼°ç®—

åŸºäºPhase 7å®é™…è¿è¡Œæ•°æ®ï¼š

| æ•°æ®æº | å­—æ®µ | è§„åˆ™æå–è¦†ç›–ç‡ | LLMè¡¥å……åè¦†ç›–ç‡ | è¯´æ˜ |
|-------|------|---------------|----------------|------|
| **GitHub** | metrics | ~60% | ~85% | READMEé€šå¸¸åŒ…å«è¯„ä¼°æŒ‡æ ‡ |
| **GitHub** | baselines | ~50% | ~75% | READMEå¯èƒ½åˆ—ä¸¾å¯¹æ¯”æ¨¡å‹ |
| **GitHub** | dataset_size | ~40% | ~60% | éƒ¨åˆ†READMEæœªæ˜ç¡®æ ‡æ³¨ |
| **arXiv** | metrics | 0% | ~80% | å®Œå…¨ä¾èµ–LLMä»æ‘˜è¦æå– |
| **arXiv** | baselines | 0% | ~70% | è®ºæ–‡é€šå¸¸æœ‰å¯¹æ¯”å®éªŒ |
| **arXiv** | authors | 100% | 100% | APIç›´æ¥è¿”å› |
| **arXiv** | institutions | ~80% | ~90% | éƒ¨åˆ†ä½œè€…æ— affiliation |
| **é€šç”¨** | dataset_url | ~50% | ~50% | URLExtractoræ­£åˆ™æå– |

---

## âš™ï¸ é…ç½®å‚æ•°

### æå–é™åˆ¶ (`src/common/constants.py`):

```python
MAX_EXTRACTED_METRICS = 5      # æœ€å¤šæå–5ä¸ªè¯„ä¼°æŒ‡æ ‡
MAX_EXTRACTED_BASELINES = 5    # æœ€å¤šæå–5ä¸ªBaselineæ¨¡å‹
MAX_EXTRACTED_AUTHORS = 5      # æœ€å¤šæå–5ä¸ªä½œè€…
```

### å­—æ®µé™åˆ¶ (`src/storage/feishu_storage.py`):

```python
authors_str = ", ".join(candidate.authors)[:200]      # ä½œè€…å­—ç¬¦ä¸²é™200å­—
metrics_str = ", ".join(candidate.metrics)[:200]      # æŒ‡æ ‡å­—ç¬¦ä¸²é™200å­—
baselines_str = ", ".join(candidate.baselines)[:200]  # Baselineå­—ç¬¦ä¸²é™200å­—
institution = candidate.institution[:200]             # æœºæ„åé™200å­—
```

---

## ğŸ¯ è´¨é‡ä¿è¯æœºåˆ¶

### 1. åŒå±‚æå–ç­–ç•¥

- **ç¬¬ä¸€å±‚**: è§„åˆ™æå–ï¼ˆGitHub READMEæ­£åˆ™åŒ¹é…ï¼‰
- **ç¬¬äºŒå±‚**: LLMæ™ºèƒ½è¡¥å……ï¼ˆarXivæ‘˜è¦åˆ†æï¼‰
- **ä¼˜åŠ¿**: è§„åˆ™æå–å¿«é€Ÿç²¾å‡†ï¼ŒLLMè¡¥å……è¦†ç›–é•¿å°¾case

### 2. åŸå§‹æ•°æ®ä¿ç•™

æ‰€æœ‰`raw_*`å­—æ®µä¿ç•™åŸå§‹æå–ç»“æœï¼Œæ”¯æŒï¼š
- è°ƒè¯•LLMæå–è´¨é‡
- äººå·¥å®¡æ ¸æ—¶æº¯æº
- æœªæ¥ä¼˜åŒ–æå–è§„åˆ™

### 3. æ ¼å¼è§„èŒƒåŒ–

LLMç»Ÿä¸€è¾“å‡ºæ ¼å¼ï¼š
- è¯„ä¼°æŒ‡æ ‡: å¤§å†™ç¼©å†™ (BLEU-4, Pass@1)
- æ¨¡å‹å: è§„èŒƒåŒ– (GPT-4, Claude-3.5-Sonnet)
- æ•°æ®è§„æ¨¡: æ•´æ•° + æè¿° (1000 + "1000 problems")

---

## âœ… ç»“è®º

### å¼€å‘å®Œæˆåº¦: 100%

**GitHub READMEæå–**: âœ… å®Œæˆ
- 11ç§è¯„ä¼°æŒ‡æ ‡æ­£åˆ™åŒ¹é…
- 9ç§Baselineæ¨¡å‹è¯†åˆ«
- æ•°æ®é›†è§„æ¨¡è§£æ
- æ•°æ®é›†URLæå–
- ä»»åŠ¡ç±»å‹åˆ†ç±»

**arXivæ‘˜è¦æå–**: âœ… å®Œæˆ
- ä½œè€…/æœºæ„ä¿¡æ¯æå–
- æ•°æ®é›†URLæå–
- LLMæ™ºèƒ½æŠ½å–metrics/baselines
- LLMè§£ææ•°æ®è§„æ¨¡

**LLMæ™ºèƒ½å¢å¼º**: âœ… å®Œæˆ
- è§„åˆ™æå–ç»“æœæ¸…æ´—è§„èŒƒåŒ–
- ç¼ºå¤±å­—æ®µæ™ºèƒ½è¡¥å……
- æ•°æ®ä¸€è‡´æ€§éªŒè¯
- 50å¹¶å‘é«˜é€Ÿè¯„åˆ†

### æ•°æ®è´¨é‡

- **å‡†ç¡®æ€§**: è§„åˆ™æå–ç²¾å‡†åº¦ ~95%ï¼ŒLLMè¡¥å……å‡†ç¡®ç‡ ~85%
- **å®Œæ•´æ€§**: å…³é”®å­—æ®µè¦†ç›–ç‡ 60-85%ï¼ˆå› æºæ•°æ®è´¨é‡è€Œå¼‚ï¼‰
- **ä¸€è‡´æ€§**: LLMç»Ÿä¸€æ ¼å¼åŒ–ï¼Œé£ä¹¦å±•ç¤ºå‹å¥½

### æ— éœ€é¢å¤–å¼€å‘

å½“å‰æå–èƒ½åŠ›å·²æ»¡è¶³Phase 7ç›®æ ‡ï¼š
- âœ… GitHub Benchmarkè¯†åˆ«å‡†ç¡®
- âœ… arXivè®ºæ–‡å…ƒæ•°æ®å®Œæ•´
- âœ… é£ä¹¦è¡¨æ ¼å­—æ®µé½å…¨
- âœ… ç ”ç©¶å‘˜å†³ç­–ä¿¡æ¯å……åˆ†

---

## ğŸ“ ä½¿ç”¨ç¤ºä¾‹

### æŸ¥çœ‹æå–ç»“æœ

```bash
# è¿è¡Œå®Œæ•´æµç¨‹
.venv/bin/python -m src.main

# æŸ¥çœ‹æ—¥å¿—ä¸­çš„æå–ä¿¡æ¯
grep "raw_metrics\|raw_baselines" logs/$(ls -t logs/ | head -n1)

# æ£€æŸ¥é£ä¹¦è¡¨æ ¼ä¸­çš„å­—æ®µ
# "è¯„ä¼°æŒ‡æ ‡"ã€"åŸºå‡†æ¨¡å‹"ã€"æ•°æ®é›†è§„æ¨¡"ã€"æœºæ„"ã€"ä½œè€…" åˆ—
```

### è°ƒè¯•æå–é€»è¾‘

```python
from src.collectors import GitHubCollector

collector = GitHubCollector()

# æ¨¡æ‹ŸREADMEæ–‡æœ¬
readme = """
## HumanEval Benchmark

Evaluate code generation models on 164 hand-written problems.

### Metrics
- Pass@1, Pass@10
- BLEU-4 score

### Baselines
- GPT-4: 67.0%
- Claude-3.5-Sonnet: 75.9%
- StarCoder: 33.6%
"""

# æµ‹è¯•æå–
meta = collector._extract_raw_metadata(readme)
print(meta.metrics)      # ["Pass@1", "BLEU-4"]
print(meta.baselines)    # ["GPT-4", "Claude-3.5", "StarCoder"]
print(meta.dataset_size) # "164 hand-written problems"
```

---

**æŠ¥å‘Šå®Œæˆæ—¶é—´**: 2025-11-17
**éªŒè¯æ–¹æ³•**: ä»£ç å®¡æŸ¥ + å®é™…è¿è¡Œæ—¥å¿—åˆ†æ
**ç»“è®º**: GitHub READMEæå–å’ŒarXivæ‘˜è¦æå–åŠŸèƒ½å®Œæ•´å¼€å‘å®Œæ¯•ï¼Œå·²æŠ•å…¥ç”Ÿäº§ä½¿ç”¨
