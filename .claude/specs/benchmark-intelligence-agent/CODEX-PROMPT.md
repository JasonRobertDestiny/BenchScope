# BenchScope MVPå¼€å‘ä»»åŠ¡ - Codexæ‰§è¡ŒæŒ‡ä»¤

ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„Pythonå¼€å‘å·¥ç¨‹å¸ˆï¼Œç°åœ¨éœ€è¦å¼€å‘**BenchScope** - ä¸€ä¸ªBenchmarkæƒ…æŠ¥è‡ªåŠ¨åŒ–ç³»ç»Ÿã€‚

---

## é¡¹ç›®ä¸Šä¸‹æ–‡

### ä¸šåŠ¡èƒŒæ™¯
- **é—®é¢˜**: ç ”ç©¶å›¢é˜Ÿæ‰‹åŠ¨ç­›é€‰AI/Agenté¢†åŸŸBenchmarkï¼Œæ¯æœˆé˜…è¯»200+ç¯‡è®ºæ–‡ï¼Œè€—æ—¶16å°æ—¶
- **æ–¹æ¡ˆ**: è‡ªåŠ¨åŒ–ç³»ç»Ÿæ¯æ—¥é‡‡é›†ã€æ™ºèƒ½è¯„åˆ†ã€æ¨é€é«˜è´¨é‡å€™é€‰
- **ROI**: èŠ‚çœÂ¥12,750/å‘¨äººåŠ›æˆæœ¬ï¼ŒBenchmarkå‘ç°é€Ÿåº¦ä»2-3ä¸ª/æœˆæå‡åˆ°10-20ä¸ª/æœˆ

### æŠ€æœ¯æ ˆ
- **è¯­è¨€**: Python 3.11+
- **å¼‚æ­¥æ¡†æ¶**: asyncio + httpx
- **LLM**: OpenAI gpt-4o-mini (æˆæœ¬ä¼˜åŒ–: Â¥1/æœˆ)
- **ç¼“å­˜**: Redis (7å¤©TTL, 30%å‘½ä¸­ç‡)
- **å­˜å‚¨**: é£ä¹¦å¤šç»´è¡¨æ ¼(ä¸») + SQLite(é™çº§å¤‡ä»½)
- **è°ƒåº¦**: GitHub Actions (æ¯æ—¥UTC 2:00)
- **é€šçŸ¥**: é£ä¹¦Webhook

### æ¶æ„è®¾è®¡(å·²å®Œæˆ,94/100è´¨é‡)
```
GitHub Actions Cron
  â†“
main.py ç¼–æ’å™¨
  â†“
å¹¶å‘é‡‡é›† (asyncio.gather)
  â”œâ”€ ArxivCollector (10s timeout, 3 retries)
  â”œâ”€ GitHubCollector (5s timeout)
  â””â”€ PwCCollector (15s timeout)
  â†“
è§„åˆ™é¢„ç­›é€‰ (è¿‡æ»¤50%å™ªéŸ³)
  â†“
LLMè¯„åˆ† (gpt-4o-mini + Redisç¼“å­˜7å¤©)
  â”œâ”€ 5ç»´åº¦: åˆ›æ–°æ€§/æŠ€æœ¯æ·±åº¦/å½±å“åŠ›/æ•°æ®è´¨é‡/å¯å¤ç°æ€§
  â””â”€ Fallback: è§„åˆ™è¯„åˆ†
  â†“
å­˜å‚¨ç®¡ç†å™¨
  â”œâ”€ Primary: é£ä¹¦å¤šç»´è¡¨æ ¼ (æ‰¹é‡å†™å…¥20æ¡/è¯·æ±‚)
  â””â”€ Fallback: SQLite (7å¤©è‡ªåŠ¨åŒæ­¥)
  â†“
é£ä¹¦é€šçŸ¥ (Webhookæ¨é€Top 5)
```

---

## å·²å®Œæˆè®¾è®¡æ–‡æ¡£

ä½ å¯ä»¥åœ¨ä»¥ä¸‹ä½ç½®æŸ¥é˜…å®Œæ•´è®¾è®¡:

1. **`.claude/specs/benchmark-intelligence-agent/01-product-requirements.md`**
   - 93/100è´¨é‡åˆ†
   - 14ä¸ªç”¨æˆ·æ•…äº‹ï¼Œå®Œæ•´éªŒæ”¶æ ‡å‡†

2. **`.claude/specs/benchmark-intelligence-agent/02-system-architecture.md`**
   - 94/100è´¨é‡åˆ†
   - å®Œæ•´ä»£ç ç¤ºä¾‹ã€é”™è¯¯å¤„ç†ç­–ç•¥ã€æ€§èƒ½åˆ†æ

3. **`.claude/specs/benchmark-intelligence-agent/CODEX-DEVELOPMENT-BRIEF.md`**
   - MVPå®æ–½æ¸…å•
   - ä»£ç è§„èŒƒã€æµ‹è¯•è¦æ±‚

---

## MVPå¼€å‘ä»»åŠ¡æ¸…å•

### ç›®å½•ç»“æ„
```
BenchScope/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ models.py                    # æ•°æ®æ¨¡å‹
â”‚   â”œâ”€â”€ config.py                    # é…ç½®ç®¡ç†
â”‚   â”œâ”€â”€ collectors/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ arxiv_collector.py       # arXivé‡‡é›†å™¨
â”‚   â”‚   â”œâ”€â”€ github_collector.py      # GitHub Trendingé‡‡é›†å™¨
â”‚   â”‚   â””â”€â”€ pwc_collector.py         # Papers with Codeé‡‡é›†å™¨
â”‚   â”œâ”€â”€ prefilter/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ rule_filter.py           # è§„åˆ™é¢„ç­›é€‰
â”‚   â”œâ”€â”€ scorer/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ llm_scorer.py            # LLMè¯„åˆ†å™¨
â”‚   â”‚   â””â”€â”€ rule_scorer.py           # è§„åˆ™è¯„åˆ†å™¨
â”‚   â”œâ”€â”€ storage/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ feishu_storage.py        # é£ä¹¦å­˜å‚¨
â”‚   â”‚   â”œâ”€â”€ sqlite_fallback.py       # SQLiteé™çº§
â”‚   â”‚   â””â”€â”€ storage_manager.py       # å­˜å‚¨ç®¡ç†å™¨
â”‚   â”œâ”€â”€ notifier/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ feishu_notifier.py       # é£ä¹¦é€šçŸ¥
â”‚   â””â”€â”€ main.py                      # ä¸»ç¼–æ’å™¨
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ unit/
â”‚       â”œâ”€â”€ test_scorer.py
â”‚       â””â”€â”€ test_storage.py
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â””â”€â”€ daily_collect.yml        # GitHub Actionså·¥ä½œæµ
â”œâ”€â”€ logs/                            # æ—¥å¿—ç›®å½•
â”œâ”€â”€ .env.example                     # ç¯å¢ƒå˜é‡æ¨¡æ¿
â”œâ”€â”€ requirements.txt                 # ä¾èµ–æ¸…å•
â””â”€â”€ README.md                        # é¡¹ç›®è¯´æ˜
```

---

## å®æ–½æ­¥éª¤ (ä¸¥æ ¼æŒ‰æ­¤é¡ºåº)

### Step 1: åŸºç¡€æ¨¡å— (30åˆ†é’Ÿ)

#### 1.1 æ•°æ®æ¨¡å‹ (`src/models.py`)
```python
from dataclasses import dataclass
from datetime import datetime
from typing import List, Optional

@dataclass
class RawCandidate:
    """é‡‡é›†å™¨åŸå§‹è¾“å‡º"""
    title: str
    url: str
    source: str  # 'arxiv' | 'github' | 'pwc'
    abstract: Optional[str] = None
    authors: Optional[List[str]] = None
    publish_date: Optional[datetime] = None
    github_stars: Optional[int] = None
    github_url: Optional[str] = None
    dataset_url: Optional[str] = None
    raw_metadata: Optional[dict] = None

@dataclass
class BenchmarkScore:
    """5ç»´åº¦è¯„åˆ†"""
    innovation: int        # 0-10: åˆ›æ–°æ€§
    technical_depth: int   # 0-10: æŠ€æœ¯æ·±åº¦
    impact: int           # 0-10: å½±å“åŠ›
    data_quality: int     # 0-10: æ•°æ®è´¨é‡
    reproducibility: int  # 0-10: å¯å¤ç°æ€§

    @property
    def total_score(self) -> int:
        return sum([
            self.innovation, self.technical_depth,
            self.impact, self.data_quality, self.reproducibility
        ])

    @property
    def priority(self) -> str:
        """ä¼˜å…ˆçº§åˆ†çº§"""
        if self.total_score >= 40:
            return "high"
        elif self.total_score >= 30:
            return "medium"
        else:
            return "low"

@dataclass
class ScoredCandidate:
    """è¯„åˆ†åçš„å€™é€‰"""
    raw: RawCandidate
    score: BenchmarkScore
    filter_reason: Optional[str] = None
```

#### 1.2 é…ç½®ç®¡ç† (`src/config.py`)
```python
import os
from dotenv import load_dotenv

load_dotenv('.env.local')

class Config:
    # OpenAI
    OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')
    OPENAI_MODEL = os.getenv('OPENAI_MODEL', 'gpt-4o-mini')

    # é£ä¹¦
    FEISHU_APP_ID = os.getenv('FEISHU_APP_ID')
    FEISHU_APP_SECRET = os.getenv('FEISHU_APP_SECRET')
    FEISHU_BITABLE_APP_TOKEN = os.getenv('FEISHU_BITABLE_APP_TOKEN')
    FEISHU_BITABLE_TABLE_ID = os.getenv('FEISHU_BITABLE_TABLE_ID')
    FEISHU_WEBHOOK_URL = os.getenv('FEISHU_WEBHOOK_URL')

    # Redis
    REDIS_URL = os.getenv('REDIS_URL', 'redis://localhost:6379')

    # æ—¥å¿—
    LOG_LEVEL = os.getenv('LOG_LEVEL', 'INFO')
    LOG_DIR = 'logs/'

    @classmethod
    def validate(cls):
        """éªŒè¯å¿…éœ€é…ç½®"""
        required = ['OPENAI_API_KEY', 'FEISHU_APP_ID', 'FEISHU_APP_SECRET']
        missing = [k for k in required if not getattr(cls, k)]
        if missing:
            raise ValueError(f"ç¼ºå°‘å¿…éœ€é…ç½®: {missing}")
```

### Step 2: æ•°æ®é‡‡é›†å™¨ (1å°æ—¶)

**ä¼˜å…ˆçº§**: ArxivCollector > GitHubCollector > PwCCollector

#### 2.1 ArxivCollector (`src/collectors/arxiv_collector.py`)
```python
import arxiv
import asyncio
import logging
from typing import List
from datetime import datetime, timedelta
from ..models import RawCandidate

logger = logging.getLogger('BenchScope.ArxivCollector')

class ArxivCollector:
    """arXivè®ºæ–‡é‡‡é›†å™¨"""

    def __init__(self):
        self.keywords = ["benchmark", "agent evaluation", "code generation", "web automation"]
        self.categories = ["cs.AI", "cs.CL", "cs.SE"]
        self.max_results = 50
        self.timeout = 10

    async def collect(self) -> List[RawCandidate]:
        """é‡‡é›†æœ€è¿‘24å°æ—¶çš„è®ºæ–‡"""

        # æ„å»ºæŸ¥è¯¢
        query_parts = [f'all:"{kw}"' for kw in self.keywords]
        query = " OR ".join(query_parts)

        cat_filter = " OR ".join([f"cat:{c}" for c in self.categories])
        full_query = f"({query}) AND ({cat_filter})"

        try:
            search = arxiv.Search(
                query=full_query,
                max_results=self.max_results,
                sort_by=arxiv.SortCriterion.SubmittedDate,
                sort_order=arxiv.SortOrder.Descending
            )

            loop = asyncio.get_event_loop()
            results = await asyncio.wait_for(
                loop.run_in_executor(None, list, search.results()),
                timeout=self.timeout
            )

        except asyncio.TimeoutError:
            logger.error(f"arXivé‡‡é›†è¶…æ—¶({self.timeout}s)")
            return []
        except Exception as e:
            logger.error(f"arXivé‡‡é›†å¤±è´¥: {e}")
            return []

        # è½¬æ¢ä¸ºRawCandidate
        candidates = []
        cutoff_date = datetime.now() - timedelta(days=1)

        for paper in results:
            if paper.published < cutoff_date:
                continue

            candidates.append(RawCandidate(
                title=paper.title,
                url=paper.pdf_url,
                source='arxiv',
                abstract=paper.summary,
                authors=[a.name for a in paper.authors],
                publish_date=paper.published,
                raw_metadata={
                    'arxiv_id': paper.entry_id.split('/')[-1],
                    'categories': paper.categories
                }
            ))

        logger.info(f"arXivé‡‡é›†å®Œæˆ,å‘ç°{len(candidates)}ç¯‡æ–°è®ºæ–‡")
        return candidates
```

#### 2.2 GitHubCollector (`src/collectors/github_collector.py`)
```python
import httpx
from bs4 import BeautifulSoup
import logging
from typing import List
from datetime import datetime
from ..models import RawCandidate

logger = logging.getLogger('BenchScope.GitHubCollector')

class GitHubCollector:
    """GitHub Trendingé‡‡é›†å™¨"""

    def __init__(self):
        self.min_stars = 100
        self.timeout = 5
        self.base_url = "https://github.com/trending"

    async def collect(self) -> List[RawCandidate]:
        """é‡‡é›†GitHub Trendingä»“åº“"""

        candidates = []

        try:
            async with httpx.AsyncClient(timeout=self.timeout) as client:
                resp = await client.get(self.base_url)
                resp.raise_for_status()

                soup = BeautifulSoup(resp.text, 'html.parser')
                repos = soup.find_all('article', class_='Box-row')

                for repo in repos:
                    h2 = repo.find('h2')
                    if not h2:
                        continue

                    repo_name = h2.get_text(strip=True).replace(' ', '').replace('\n', '')
                    repo_url = f"https://github.com/{repo_name}"

                    # æå–staræ•°
                    star_elem = repo.find('svg', {'aria-label': 'star'})
                    stars = 0
                    if star_elem:
                        stars_text = star_elem.find_next('span').get_text(strip=True)
                        stars = self._parse_stars(stars_text)

                    if stars < self.min_stars:
                        continue

                    # æå–æè¿°
                    desc_elem = repo.find('p', class_='col-9')
                    description = desc_elem.get_text(strip=True) if desc_elem else ""

                    candidates.append(RawCandidate(
                        title=repo_name,
                        url=repo_url,
                        source='github',
                        abstract=description,
                        github_stars=stars,
                        github_url=repo_url,
                        publish_date=datetime.now()
                    ))

        except httpx.TimeoutException:
            logger.error(f"GitHub Trendingé‡‡é›†è¶…æ—¶")
            return []
        except Exception as e:
            logger.error(f"GitHub Trendingé‡‡é›†å¤±è´¥: {e}")
            return []

        logger.info(f"GitHubé‡‡é›†å®Œæˆ,å‘ç°{len(candidates)}ä¸ªtrendingä»“åº“")
        return candidates

    def _parse_stars(self, stars_text: str) -> int:
        """è§£æstaræ•°é‡: '1.2k' -> 1200"""
        stars_text = stars_text.replace(',', '')
        if 'k' in stars_text:
            return int(float(stars_text.replace('k', '')) * 1000)
        return int(stars_text)
```

#### 2.3 PwCCollector - æš‚æ—¶è¿”å›ç©ºåˆ—è¡¨(Phase 2å®ç°)
```python
# src/collectors/pwc_collector.py
import logging
from typing import List
from ..models import RawCandidate

logger = logging.getLogger('BenchScope.PwCCollector')

class PwCCollector:
    """Papers with Codeé‡‡é›†å™¨ (Phase 2å®ç°)"""

    async def collect(self) -> List[RawCandidate]:
        """æš‚æ—¶è¿”å›ç©ºåˆ—è¡¨"""
        logger.info("PwCé‡‡é›†å™¨æš‚æœªå®ç°,è¿”å›ç©ºåˆ—è¡¨")
        return []
```

### Step 3: é¢„ç­›é€‰å™¨ (30åˆ†é’Ÿ)

#### 3.1 è§„åˆ™é¢„ç­›é€‰ (`src/prefilter/rule_filter.py`)
```python
import logging
from typing import List
from ..models import RawCandidate

logger = logging.getLogger('BenchScope.RuleFilter')

class RuleBasedPrefilter:
    """è§„åˆ™é¢„ç­›é€‰å™¨"""

    def __init__(self):
        self.min_stars = 50

    def filter(self, candidates: List[RawCandidate]) -> List[RawCandidate]:
        """è§„åˆ™è¿‡æ»¤"""

        filtered = []

        for c in candidates:
            # è§„åˆ™1: GitHubä»“åº“staræ•°è¿‡ä½
            if c.source == 'github' and (c.github_stars or 0) < self.min_stars:
                logger.debug(f"è¿‡æ»¤: {c.title} - staræ•°è¿‡ä½({c.github_stars})")
                continue

            # è§„åˆ™2: arXivè®ºæ–‡æ— æ‘˜è¦
            if c.source == 'arxiv' and not c.abstract:
                logger.debug(f"è¿‡æ»¤: {c.title} - æ— æ‘˜è¦")
                continue

            # è§„åˆ™3: æ ‡é¢˜å«"survey"ä½†æ— ä»£ç 
            if 'survey' in c.title.lower() and not c.github_url:
                logger.debug(f"è¿‡æ»¤: {c.title} - surveyç±»è®ºæ–‡æ— ä»£ç ")
                continue

            filtered.append(c)

        filter_rate = (1 - len(filtered) / len(candidates)) * 100 if candidates else 0
        logger.info(f"é¢„ç­›é€‰å®Œæˆ,è¿‡æ»¤ç‡{filter_rate:.1f}%,å‰©ä½™{len(filtered)}æ¡")

        return filtered
```

### Step 4: è¯„åˆ†å™¨ (1.5å°æ—¶)

#### 4.1 è§„åˆ™è¯„åˆ†å™¨ (`src/scorer/rule_scorer.py`)
```python
from ..models import BenchmarkScore, RawCandidate

class RuleScorer:
    """è§„åˆ™è¯„åˆ†å™¨(LLM fallback)"""

    def score(self, candidate: RawCandidate) -> BenchmarkScore:
        """åŸºäºGitHub starsç²—ç•¥è¯„åˆ†"""

        stars = candidate.github_stars or 0

        if stars >= 1000:
            base_score = 8
        elif stars >= 500:
            base_score = 6
        elif stars >= 100:
            base_score = 4
        else:
            base_score = 2

        return BenchmarkScore(
            innovation=base_score,
            technical_depth=base_score,
            impact=base_score,
            data_quality=base_score,
            reproducibility=base_score if candidate.github_url else 2
        )
```

#### 4.2 LLMè¯„åˆ†å™¨ (`src/scorer/llm_scorer.py`)
```python
import openai
import redis
import json
import hashlib
import logging
from typing import Optional
from tenacity import retry, stop_after_attempt, wait_exponential
from ..models import BenchmarkScore, RawCandidate
from ..config import Config
from .rule_scorer import RuleScorer

logger = logging.getLogger('BenchScope.LLMScorer')

class LLMScorer:
    """LLMæ™ºèƒ½è¯„åˆ†å™¨"""

    def __init__(self):
        self.client = openai.AsyncOpenAI(api_key=Config.OPENAI_API_KEY)
        self.model = Config.OPENAI_MODEL
        self.timeout = 30
        self.cache = redis.Redis.from_url(Config.REDIS_URL, decode_responses=True)
        self.cache_ttl = 7 * 24 * 3600  # 7å¤©
        self.fallback_scorer = RuleScorer()

    async def score(self, candidate: RawCandidate) -> BenchmarkScore:
        """ä¸ºå€™é€‰æ‰“åˆ†"""

        # æ£€æŸ¥ç¼“å­˜
        cache_key = self._get_cache_key(candidate)
        cached = self.cache.get(cache_key)
        if cached:
            logger.debug(f"ç¼“å­˜å‘½ä¸­: {candidate.title[:50]}")
            return BenchmarkScore(**json.loads(cached))

        # æ„å»ºPrompt
        prompt = self._build_prompt(candidate)

        # è°ƒç”¨LLM
        try:
            score = await self._call_llm(prompt)
        except Exception as e:
            logger.error(f"LLMè°ƒç”¨å¤±è´¥,fallbackåˆ°è§„åˆ™è¯„åˆ†: {e}")
            score = self.fallback_scorer.score(candidate)

        # å†™å…¥ç¼“å­˜
        self.cache.setex(cache_key, self.cache_ttl, json.dumps(score.__dict__))

        return score

    def _build_prompt(self, candidate: RawCandidate) -> str:
        """æ„å»ºè¯„åˆ†Prompt"""
        return f"""
è¯·å¯¹ä»¥ä¸‹Benchmark/è¯„æµ‹æ•°æ®é›†è¿›è¡Œ5ç»´åº¦è¯„åˆ†(æ¯ä¸ªç»´åº¦0-10åˆ†):

æ ‡é¢˜: {candidate.title}
æ¥æº: {candidate.source}
æ‘˜è¦: {candidate.abstract or 'N/A'}
GitHub Stars: {candidate.github_stars or 'N/A'}

è¯„åˆ†ç»´åº¦:
1. åˆ›æ–°æ€§ (Innovation): ä»»åŠ¡æˆ–æ–¹æ³•çš„æ–°é¢–æ€§
2. æŠ€æœ¯æ·±åº¦ (Technical Depth): æŠ€æœ¯å¤æ‚åº¦,å­¦æœ¯ä»·å€¼
3. å½±å“åŠ› (Impact): åœ¨AI/Agenté¢†åŸŸçš„æ½œåœ¨å½±å“åŠ›
4. æ•°æ®è´¨é‡ (Data Quality): æ•°æ®é›†è§„æ¨¡ã€å¤šæ ·æ€§
5. å¯å¤ç°æ€§ (Reproducibility): ä»£ç /æ•°æ®å¼€æºç¨‹åº¦

è¯·ä»¥JSONæ ¼å¼è¿”å›:
{{
    "innovation": <0-10>,
    "technical_depth": <0-10>,
    "impact": <0-10>,
    "data_quality": <0-10>,
    "reproducibility": <0-10>
}}
""".strip()

    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=1, max=10))
    async def _call_llm(self, prompt: str) -> BenchmarkScore:
        """è°ƒç”¨LLM API"""

        response = await self.client.chat.completions.create(
            model=self.model,
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªAI Benchmarkè¯„ä¼°ä¸“å®¶ã€‚"},
                {"role": "user", "content": prompt}
            ],
            temperature=0.3,
            max_tokens=200,
            timeout=self.timeout
        )

        content = response.choices[0].message.content
        data = json.loads(content)

        return BenchmarkScore(
            innovation=data['innovation'],
            technical_depth=data['technical_depth'],
            impact=data['impact'],
            data_quality=data['data_quality'],
            reproducibility=data['reproducibility']
        )

    def _get_cache_key(self, candidate: RawCandidate) -> str:
        """ç”Ÿæˆç¼“å­˜key"""
        return f"score:{hashlib.md5(candidate.title.encode()).hexdigest()}"
```

### Step 5: å­˜å‚¨å±‚ (1.5å°æ—¶)

#### 5.1 SQLiteé™çº§å¤‡ä»½ (`src/storage/sqlite_fallback.py`)
```python
import sqlite3
import json
import logging
from datetime import datetime, timedelta
from typing import List
from ..models import ScoredCandidate, RawCandidate, BenchmarkScore

logger = logging.getLogger('BenchScope.SQLiteFallback')

class SQLiteFallback:
    """SQLiteé™çº§å­˜å‚¨"""

    def __init__(self, db_path='fallback.db'):
        self.db_path = db_path
        self._init_db()

    def _init_db(self):
        """åˆå§‹åŒ–æ•°æ®åº“"""
        conn = sqlite3.connect(self.db_path)
        conn.execute("""
            CREATE TABLE IF NOT EXISTS fallback_candidates (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                title TEXT NOT NULL,
                source TEXT NOT NULL,
                url TEXT UNIQUE NOT NULL,
                score_json TEXT NOT NULL,
                raw_json TEXT NOT NULL,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                synced_to_feishu BOOLEAN DEFAULT 0
            )
        """)
        conn.commit()
        conn.close()

    async def save(self, candidates: List[ScoredCandidate]):
        """ä¿å­˜åˆ°SQLite"""
        conn = sqlite3.connect(self.db_path)

        for c in candidates:
            try:
                conn.execute(
                    """INSERT OR IGNORE INTO fallback_candidates
                       (title, source, url, score_json, raw_json)
                       VALUES (?, ?, ?, ?, ?)""",
                    (
                        c.raw.title,
                        c.raw.source,
                        c.raw.url,
                        json.dumps(c.score.__dict__),
                        json.dumps({
                            'title': c.raw.title,
                            'url': c.raw.url,
                            'source': c.raw.source,
                            'abstract': c.raw.abstract,
                            'github_stars': c.raw.github_stars,
                            'github_url': c.raw.github_url
                        })
                    )
                )
            except Exception as e:
                logger.error(f"SQLiteå†™å…¥å¤±è´¥: {c.raw.title} - {e}")

        conn.commit()
        conn.close()
        logger.info(f"SQLiteå¤‡ä»½å®Œæˆ: {len(candidates)}æ¡")

    async def get_unsynced(self) -> List[ScoredCandidate]:
        """è·å–æœªåŒæ­¥è®°å½•"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.execute(
            "SELECT score_json, raw_json FROM fallback_candidates WHERE synced_to_feishu = 0"
        )

        candidates = []
        for row in cursor:
            score_data = json.loads(row[0])
            raw_data = json.loads(row[1])

            candidates.append(ScoredCandidate(
                raw=RawCandidate(**raw_data),
                score=BenchmarkScore(**score_data)
            ))

        conn.close()
        return candidates

    async def mark_synced(self, urls: List[str]):
        """æ ‡è®°å·²åŒæ­¥"""
        conn = sqlite3.connect(self.db_path)
        for url in urls:
            conn.execute(
                "UPDATE fallback_candidates SET synced_to_feishu = 1 WHERE url = ?",
                (url,)
            )
        conn.commit()
        conn.close()

    async def cleanup_old_records(self, days=7):
        """æ¸…ç†æ—§è®°å½•"""
        cutoff = datetime.now() - timedelta(days=days)
        conn = sqlite3.connect(self.db_path)
        conn.execute(
            "DELETE FROM fallback_candidates WHERE synced_to_feishu = 1 AND created_at < ?",
            (cutoff,)
        )
        conn.commit()
        conn.close()
        logger.info(f"æ¸…ç†{days}å¤©å‰çš„å·²åŒæ­¥SQLiteè®°å½•")
```

#### 5.2 é£ä¹¦å­˜å‚¨ (`src/storage/feishu_storage.py`)
```python
import httpx
import asyncio
import logging
from typing import List
from datetime import datetime, timedelta
from ..models import ScoredCandidate
from ..config import Config

logger = logging.getLogger('BenchScope.FeishuStorage')

class FeishuStorage:
    """é£ä¹¦å¤šç»´è¡¨æ ¼å­˜å‚¨"""

    def __init__(self):
        self.app_id = Config.FEISHU_APP_ID
        self.app_secret = Config.FEISHU_APP_SECRET
        self.app_token = Config.FEISHU_BITABLE_APP_TOKEN
        self.table_id = Config.FEISHU_BITABLE_TABLE_ID
        self.base_url = "https://open.feishu.cn/open-apis"
        self.batch_size = 20
        self.access_token = None
        self.token_expires_at = None

    async def save(self, candidates: List[ScoredCandidate]) -> bool:
        """æ‰¹é‡ä¿å­˜åˆ°é£ä¹¦"""

        await self._ensure_access_token()

        # æ„å»ºè®°å½•
        records = []
        for c in candidates:
            records.append({
                "fields": {
                    "æ ‡é¢˜": c.raw.title,
                    "æ¥æº": c.raw.source,
                    "URL": c.raw.url,
                    "æ‘˜è¦": c.raw.abstract or "",
                    "åˆ›æ–°æ€§": c.score.innovation,
                    "æŠ€æœ¯æ·±åº¦": c.score.technical_depth,
                    "å½±å“åŠ›": c.score.impact,
                    "æ•°æ®è´¨é‡": c.score.data_quality,
                    "å¯å¤ç°æ€§": c.score.reproducibility,
                    "æ€»åˆ†": c.score.total_score,
                    "ä¼˜å…ˆçº§": c.score.priority,
                    "çŠ¶æ€": "å¾…å®¡é˜…",
                    "å‘ç°æ—¶é—´": datetime.now().isoformat(),
                    "GitHub Stars": c.raw.github_stars or 0,
                    "GitHub URL": c.raw.github_url or ""
                }
            })

        # åˆ†æ‰¹å†™å…¥
        url = f"{self.base_url}/bitable/v1/apps/{self.app_token}/tables/{self.table_id}/records/batch_create"

        async with httpx.AsyncClient() as client:
            for i in range(0, len(records), self.batch_size):
                batch = records[i:i+self.batch_size]

                try:
                    resp = await client.post(
                        url,
                        headers={"Authorization": f"Bearer {self.access_token}"},
                        json={"records": batch},
                        timeout=10
                    )
                    resp.raise_for_status()
                    logger.info(f"é£ä¹¦å†™å…¥æˆåŠŸ: batch {i//self.batch_size + 1}, {len(batch)}æ¡")

                except httpx.HTTPStatusError as e:
                    logger.error(f"é£ä¹¦APIé”™è¯¯: {e.response.status_code}")
                    raise FeishuAPIError(f"Batch write failed: {e}")

                await asyncio.sleep(0.6)  # é™æµ: 100 req/min

        return True

    async def _ensure_access_token(self):
        """ç¡®ä¿access_tokenæœ‰æ•ˆ"""
        now = datetime.now()

        if self.access_token and self.token_expires_at and now < self.token_expires_at:
            return

        url = f"{self.base_url}/auth/v3/tenant_access_token/internal"

        async with httpx.AsyncClient() as client:
            resp = await client.post(
                url,
                json={"app_id": self.app_id, "app_secret": self.app_secret},
                timeout=5
            )
            resp.raise_for_status()
            data = resp.json()

            self.access_token = data['tenant_access_token']
            self.token_expires_at = now + timedelta(seconds=data['expire'] - 300)

class FeishuAPIError(Exception):
    """é£ä¹¦APIå¼‚å¸¸"""
    pass
```

#### 5.3 å­˜å‚¨ç®¡ç†å™¨ (`src/storage/storage_manager.py`)
```python
import logging
from typing import List
from ..models import ScoredCandidate
from .feishu_storage import FeishuStorage, FeishuAPIError
from .sqlite_fallback import SQLiteFallback

logger = logging.getLogger('BenchScope.StorageManager')

class StorageManager:
    """å­˜å‚¨ç®¡ç†å™¨: é£ä¹¦ä¸»å­˜å‚¨ + SQLiteé™çº§"""

    def __init__(self):
        self.feishu = FeishuStorage()
        self.sqlite = SQLiteFallback()

    async def save(self, candidates: List[ScoredCandidate]) -> bool:
        """ä¿å­˜å€™é€‰"""

        try:
            # å°è¯•å†™å…¥é£ä¹¦
            await self.feishu.save(candidates)
            logger.info(f"âœ“ é£ä¹¦å†™å…¥æˆåŠŸ: {len(candidates)}æ¡")

            # æˆåŠŸåæ¸…ç†æ—§å¤‡ä»½
            await self.sqlite.cleanup_old_records(days=7)

            # åŒæ­¥æœªåŒæ­¥è®°å½•
            await self._sync_sqlite_to_feishu()

            return True

        except (FeishuAPIError, Exception) as e:
            logger.error(f"âœ— é£ä¹¦å†™å…¥å¤±è´¥,é™çº§åˆ°SQLite: {e}")

            # å†™å…¥SQLite
            await self.sqlite.save(candidates)
            logger.warning(f"å·²å¯ç”¨SQLiteé™çº§å¤‡ä»½")

            return False

    async def _sync_sqlite_to_feishu(self):
        """åŒæ­¥SQLiteæœªåŒæ­¥è®°å½•"""
        unsynced = await self.sqlite.get_unsynced()

        if not unsynced:
            return

        logger.info(f"å‘ç°{len(unsynced)}æ¡æœªåŒæ­¥SQLiteè®°å½•,å°è¯•åŒæ­¥")

        try:
            await self.feishu.save(unsynced)
            urls = [c.raw.url for c in unsynced]
            await self.sqlite.mark_synced(urls)
            logger.info(f"âœ“ SQLiteè®°å½•åŒæ­¥æˆåŠŸ: {len(unsynced)}æ¡")

        except Exception as e:
            logger.error(f"âœ— SQLiteåŒæ­¥å¤±è´¥,ä¸‹æ¬¡é‡è¯•: {e}")
```

### Step 6: é€šçŸ¥å™¨ (30åˆ†é’Ÿ)

#### 6.1 é£ä¹¦é€šçŸ¥ (`src/notifier/feishu_notifier.py`)
```python
import httpx
import logging
from typing import List
from datetime import datetime
from ..models import ScoredCandidate
from ..config import Config

logger = logging.getLogger('BenchScope.FeishuNotifier')

class FeishuNotifier:
    """é£ä¹¦é€šçŸ¥"""

    def __init__(self):
        self.webhook_url = Config.FEISHU_WEBHOOK_URL

    async def notify(self, candidates: List[ScoredCandidate]):
        """å‘é€é€šçŸ¥"""

        if not candidates:
            logger.info("æ— é«˜åˆ†å€™é€‰,è·³è¿‡é€šçŸ¥")
            return

        # æ„å»ºæ¶ˆæ¯
        message = self._build_message(candidates)

        # å‘é€Webhook
        async with httpx.AsyncClient() as client:
            try:
                resp = await client.post(
                    self.webhook_url,
                    json={"msg_type": "text", "content": {"text": message}},
                    timeout=5
                )
                resp.raise_for_status()
                logger.info(f"é£ä¹¦é€šçŸ¥å‘é€æˆåŠŸ")

            except Exception as e:
                logger.error(f"é£ä¹¦é€šçŸ¥å¤±è´¥: {e}")

    def _build_message(self, candidates: List[ScoredCandidate]) -> str:
        """æ„å»ºæ¶ˆæ¯å†…å®¹"""

        lines = [
            f"ğŸ“Š BenchScopeæ—¥æŠ¥ - {datetime.now():%Y-%m-%d}",
            "",
            f"æœ¬æ¬¡å‘ç° {len(candidates)} ä¸ªé«˜è´¨é‡Benchmarkå€™é€‰",
            "",
            "ğŸ”¥ Top 5æ¨è:"
        ]

        for i, c in enumerate(candidates[:5], 1):
            lines.append(
                f"{i}. {c.raw.title} - æ€»åˆ†{c.score.total_score} ({c.score.priority})\n"
                f"   {c.raw.url}"
            )

        return "\n".join(lines)
```

### Step 7: ä¸»ç¼–æ’å™¨ (30åˆ†é’Ÿ)

#### 7.1 ä¸»æµç¨‹ (`src/main.py`)
```python
import asyncio
import logging
from datetime import datetime
from pathlib import Path

from .models import ScoredCandidate
from .config import Config
from .collectors.arxiv_collector import ArxivCollector
from .collectors.github_collector import GitHubCollector
from .collectors.pwc_collector import PwCCollector
from .prefilter.rule_filter import RuleBasedPrefilter
from .scorer.llm_scorer import LLMScorer
from .storage.storage_manager import StorageManager
from .notifier.feishu_notifier import FeishuNotifier

# é…ç½®æ—¥å¿—
Path(Config.LOG_DIR).mkdir(exist_ok=True)
logging.basicConfig(
    level=getattr(logging, Config.LOG_LEVEL),
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(f'{Config.LOG_DIR}/{datetime.now():%Y%m%d}.log'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger('BenchScope.Main')

async def run_daily_collection():
    """æ¯æ—¥é‡‡é›†ä¸»æµç¨‹"""

    logger.info("========== BenchScopeæ¯æ—¥é‡‡é›†å¼€å§‹ ==========")

    # éªŒè¯é…ç½®
    try:
        Config.validate()
    except ValueError as e:
        logger.error(f"é…ç½®éªŒè¯å¤±è´¥: {e}")
        return

    # Step 1: å¹¶å‘é‡‡é›†
    logger.info("Step 1: æ•°æ®é‡‡é›†...")
    collectors = [ArxivCollector(), GitHubCollector(), PwCCollector()]

    results = await asyncio.gather(
        *[c.collect() for c in collectors],
        return_exceptions=True
    )

    all_candidates = []
    for i, result in enumerate(results):
        if isinstance(result, Exception):
            logger.error(f"é‡‡é›†å™¨{collectors[i].__class__.__name__}å¤±è´¥: {result}")
        else:
            all_candidates.extend(result)
            logger.info(f"{collectors[i].__class__.__name__}é‡‡é›†åˆ°{len(result)}æ¡")

    logger.info(f"é‡‡é›†å®Œæˆ,å…±{len(all_candidates)}æ¡åŸå§‹æ•°æ®")

    if not all_candidates:
        logger.warning("æ— æ•°æ®é‡‡é›†,æµç¨‹ç»“æŸ")
        return

    # Step 2: è§„åˆ™é¢„ç­›é€‰
    logger.info("Step 2: è§„åˆ™é¢„ç­›é€‰...")
    prefilter = RuleBasedPrefilter()
    filtered = prefilter.filter(all_candidates)

    # Step 3: LLMè¯„åˆ†
    logger.info("Step 3: LLMè¯„åˆ†...")
    scorer = LLMScorer()
    scored_candidates = []

    for candidate in filtered:
        try:
            score = await scorer.score(candidate)
            scored_candidates.append(ScoredCandidate(raw=candidate, score=score))
        except Exception as e:
            logger.error(f"è¯„åˆ†å¤±è´¥: {candidate.title[:50]} - {e}")

    logger.info(f"è¯„åˆ†å®Œæˆ,æˆåŠŸ{len(scored_candidates)}æ¡")

    if not scored_candidates:
        logger.warning("æ— å€™é€‰é€šè¿‡è¯„åˆ†,æµç¨‹ç»“æŸ")
        return

    # Step 4: å­˜å‚¨
    logger.info("Step 4: å­˜å‚¨...")
    storage = StorageManager()
    success = await storage.save(scored_candidates)

    # Step 5: é€šçŸ¥
    logger.info("Step 5: å‘é€é€šçŸ¥...")
    top5 = sorted(scored_candidates, key=lambda x: x.score.total_score, reverse=True)[:5]
    notifier = FeishuNotifier()
    await notifier.notify(top5)

    logger.info(f"========== æµç¨‹ç»“æŸ,å¤„ç†{len(scored_candidates)}æ¡å€™é€‰ ==========")

if __name__ == '__main__':
    asyncio.run(run_daily_collection())
```

### Step 8: GitHub Actions (15åˆ†é’Ÿ)

#### 8.1 å·¥ä½œæµ (`.github/workflows/daily_collect.yml`)
```yaml
name: BenchScope Daily Collection

on:
  schedule:
    - cron: '0 2 * * *'  # UTC 2:00 (åŒ—äº¬10:00)
  workflow_dispatch:

jobs:
  collect:
    runs-on: ubuntu-latest
    timeout-minutes: 20

    services:
      redis:
        image: redis:7-alpine
        ports:
          - 6379:6379

    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-python@v4
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install Dependencies
        run: pip install -r requirements.txt

      - name: Run Collection
        env:
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          FEISHU_APP_ID: ${{ secrets.FEISHU_APP_ID }}
          FEISHU_APP_SECRET: ${{ secrets.FEISHU_APP_SECRET }}
          FEISHU_BITABLE_APP_TOKEN: ${{ secrets.FEISHU_BITABLE_APP_TOKEN }}
          FEISHU_BITABLE_TABLE_ID: ${{ secrets.FEISHU_BITABLE_TABLE_ID }}
          FEISHU_WEBHOOK_URL: ${{ secrets.FEISHU_WEBHOOK_URL }}
          REDIS_URL: redis://localhost:6379
        run: python -m src.main

      - name: Upload Logs
        if: always()
        uses: actions/upload-artifact@v3
        with:
          name: logs
          path: logs/
          retention-days: 7
```

### Step 9: ä¾èµ–å’Œç¯å¢ƒ (15åˆ†é’Ÿ)

#### 9.1 ä¾èµ–æ¸…å• (`requirements.txt`)
```txt
python>=3.11

# HTTP
httpx>=0.25.0

# æ•°æ®é‡‡é›†
arxiv>=2.0.0
beautifulsoup4>=4.12.0
lxml>=4.9.0

# LLM
openai>=1.3.0

# ç¼“å­˜
redis>=5.0.0

# å·¥å…·
python-dotenv>=1.0.0
tenacity>=8.2.0

# æµ‹è¯•
pytest>=7.4.0
pytest-asyncio>=0.21.0
```

#### 9.2 ç¯å¢ƒå˜é‡æ¨¡æ¿ (`.env.example`)
```bash
# OpenAI API
OPENAI_API_KEY=sk-...
OPENAI_MODEL=gpt-4o-mini

# é£ä¹¦å¼€æ”¾å¹³å°
FEISHU_APP_ID=cli_...
FEISHU_APP_SECRET=...
FEISHU_BITABLE_APP_TOKEN=...
FEISHU_BITABLE_TABLE_ID=...
FEISHU_WEBHOOK_URL=https://open.feishu.cn/open-apis/bot/v2/hook/...

# Redisç¼“å­˜
REDIS_URL=redis://localhost:6379

# æ—¥å¿—é…ç½®
LOG_LEVEL=INFO
LOG_DIR=logs/
```

### Step 10: æµ‹è¯• (30åˆ†é’Ÿ)

#### 10.1 å•å…ƒæµ‹è¯• (`tests/unit/test_scorer.py`)
```python
import pytest
from src.models import RawCandidate
from src.scorer.llm_scorer import LLMScorer
from src.scorer.rule_scorer import RuleScorer

@pytest.mark.asyncio
async def test_llm_scorer_basic():
    """æµ‹è¯•LLMè¯„åˆ†åŸºæœ¬åŠŸèƒ½"""
    scorer = LLMScorer()

    candidate = RawCandidate(
        title="TestBench: A Benchmark for Testing",
        url="https://arxiv.org/abs/2024.00000",
        source="arxiv",
        abstract="A new benchmark...",
        github_stars=500
    )

    score = await scorer.score(candidate)

    assert 0 <= score.innovation <= 10
    assert 0 <= score.total_score <= 50
    assert score.priority in ['high', 'medium', 'low']

def test_rule_scorer():
    """æµ‹è¯•è§„åˆ™è¯„åˆ†"""
    scorer = RuleScorer()

    candidate = RawCandidate(
        title="Test",
        url="https://test.com",
        source="github",
        github_stars=1500
    )

    score = scorer.score(candidate)
    assert score.total_score >= 30  # é«˜staråº”è¯¥é«˜åˆ†
```

---

## ä»£ç è§„èŒƒ (å¼ºåˆ¶æ‰§è¡Œ)

### Pythoné£æ ¼
1. **PEP8åˆè§„**: 4ç©ºæ ¼ç¼©è¿›, `snake_case`å‡½æ•°/å˜é‡, `PascalCase`ç±»å
2. **ç±»å‹æ³¨è§£**: æ‰€æœ‰å‡½æ•°å¿…é¡»æœ‰ç±»å‹æ³¨è§£
3. **ä¸­æ–‡æ³¨é‡Š**: å…³é”®é€»è¾‘å¿…é¡»å†™ä¸­æ–‡æ³¨é‡Š
4. **åµŒå¥—é™åˆ¶**: æœ€å¤§3å±‚åµŒå¥—(Linusè§„åˆ™),è¶…è¿‡å¿…é¡»early return
5. **é­”æ³•æ•°å­—**: å®šä¹‰å¸¸é‡,ä¸è¦ç¡¬ç¼–ç 

### å¼‚æ­¥ç¼–ç¨‹
- ä½¿ç”¨`async/await`
- è¶…æ—¶æ§åˆ¶: `asyncio.timeout()`
- å¹¶å‘é™æµ: `asyncio.Semaphore()`
- å®¹é”™: `return_exceptions=True`

### é”™è¯¯å¤„ç†
- é‡‡é›†å™¨å¤±è´¥è¿”å›ç©ºåˆ—è¡¨,ä¸æŠ›å¼‚å¸¸
- LLMå¤±è´¥fallbackåˆ°è§„åˆ™è¯„åˆ†
- é£ä¹¦å¤±è´¥é™çº§åˆ°SQLite
- ä½¿ç”¨`tenacity`é‡è¯•

---

## éªŒæ”¶æ ‡å‡†

### åŠŸèƒ½éªŒæ”¶
- [ ] GitHub Actionsæ¯æ—¥è‡ªåŠ¨è¿è¡Œ
- [ ] arXiv/GitHub/PwCä¸‰ä¸ªé‡‡é›†å™¨å¯ç”¨
- [ ] é¢„ç­›é€‰è¿‡æ»¤ç‡40-60%
- [ ] LLMè¯„åˆ†æˆåŠŸç‡>90%
- [ ] é£ä¹¦å¤šç»´è¡¨æ ¼è‡ªåŠ¨å†™å…¥
- [ ] é£ä¹¦é€šçŸ¥æ¯æ—¥æ¨é€
- [ ] SQLiteé™çº§å¤‡ä»½å¯ç”¨
- [ ] æ—¥å¿—å®Œæ•´è®°å½•

### æ€§èƒ½éªŒæ”¶
- [ ] æ€»æ‰§è¡Œæ—¶é—´ < 20åˆ†é’Ÿ
- [ ] LLMæœˆæˆæœ¬ < Â¥50
- [ ] æ•°æ®é‡‡é›†æˆåŠŸç‡ > 95%

### è´¨é‡éªŒæ”¶
- [ ] ä»£ç é€šè¿‡`black`æ ¼å¼åŒ–
- [ ] ä»£ç é€šè¿‡`ruff check`
- [ ] å…³é”®é€»è¾‘æœ‰ä¸­æ–‡æ³¨é‡Š
- [ ] æ ¸å¿ƒæ¨¡å—æœ‰å•å…ƒæµ‹è¯•
- [ ] æ‰‹åŠ¨æµ‹è¯•æŠ¥å‘Šå®Œæ•´

---

## å…³é”®çº¦æŸ

1. **æ‰‹åŠ¨æµ‹è¯•å¼ºåˆ¶æ‰§è¡Œ**:
   - é£ä¹¦æ’­æŠ¥ã€é£ä¹¦å¤šç»´è¡¨æ ¼å¿…é¡»æ‰‹åŠ¨éªŒè¯
   - ç»“æœè®°å½•åˆ°`docs/test-report.md`å¹¶é™„æˆªå›¾

2. **Linuså“²å­¦**:
   - Is this a real problem? â†’ æ‹’ç»è¿‡åº¦å·¥ç¨‹
   - Is there a simpler way? â†’ æ°¸è¿œå¯»æ‰¾æœ€ç®€å•å®ç°
   - What will this break? â†’ MVPé›¶ç ´å

3. **æˆæœ¬æ§åˆ¶**:
   - LLMæœˆæˆæœ¬å¿…é¡»<Â¥50
   - ä¼˜å…ˆä½¿ç”¨ç¼“å­˜å’Œè§„åˆ™è¿‡æ»¤

---

## å¼€å§‹å¼€å‘

**ä½ çš„ä»»åŠ¡**:
1. ä¸¥æ ¼æŒ‰ç…§ä¸Šè¿°æ­¥éª¤å®æ–½MVP (Step 1-10)
2. éµå¾ªä»£ç è§„èŒƒå’Œæµ‹è¯•è¦æ±‚
3. å®Œæˆåè¿›è¡Œæ‰‹åŠ¨æµ‹è¯•å¹¶è®°å½•
4. æäº¤ä»£ç å‰è¿è¡Œ`black`å’Œ`ruff`

**å‚è€ƒæ–‡æ¡£**:
- `.claude/specs/benchmark-intelligence-agent/02-system-architecture.md` - å®Œæ•´æ¶æ„è®¾è®¡
- `.claude/specs/benchmark-intelligence-agent/CODEX-DEVELOPMENT-BRIEF.md` - è¯¦ç»†å¼€å‘æŒ‡å—

**é¢„ä¼°æ—¶é—´**: 6-8å°æ—¶

Good luck! å¼€å§‹ç¼–ç å§ã€‚ğŸš€
