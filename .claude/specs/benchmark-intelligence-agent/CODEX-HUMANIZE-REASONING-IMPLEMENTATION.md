# Codex开发指令：人性化推理输出优化实施

**开发人**: Codex
**指派人**: Claude Code
**优先级**: P1 - 高优先级（影响用户阅读体验）
**预计工时**: 30分钟（Prompt修改15分钟 + 测试15分钟）
**目标**: 去除AI味，提升评分依据的阅读体验

**前置文档**: `.claude/specs/benchmark-intelligence-agent/PHASE-HUMANIZE-REASONING-PRD.md`

---

## 📋 问题背景（精简版）

**用户反馈**（2025-11-21）:
> "感觉这个评分依据的描述有点AI味了，去掉【】这些是不是阅读效果要好的一些呢，同时重点的内容是不是可以加粗等等，提高阅读体验感"

**AI味问题清单**:
- 【】标记泛滥 → 像看日志
- 长段落无换行 → 难以抓住重点
- 缺少视觉层次 → 没有加粗、列表
- 只说好不说坏 → 缺乏批判性思维
- 综合推理太短 → 50字符不够

**解决方案**:
修改`src/scorer/llm_scorer.py`的LLM Prompt，增加"写作风格要求"，要求markdown格式输出。

---

## 🔧 实施步骤

### Step 1: 新增"写作风格要求"部分（第90行之后插入）

**文件路径**: `src/scorer/llm_scorer.py`

**插入位置**: 第90行（在`=== 第3部分：5维详细评分标准 ===`之前）

**插入内容**:

```python
=== 第2.5部分：写作风格要求（Phase 人性化推理优化）===

【重要】你的推理输出会直接展示给用户，必须遵循以下写作风格：

**1. 标题格式** - 使用`**标题 X/10**`，不要使用【】
❌ 错误：【活跃度】该候选项在GitHub...
✅ 正确：**活跃度 10/10** - 社区热度高，维护积极

**2. 短段落呼吸感** - 每段2-3行，不超过100字
❌ 错误：该候选项在GitHub上拥有6307个stars，显示出显著的社区关注度。最近的提交日期为2025年11月14日，表明项目仍在活跃维护中。项目的PR和Issue讨论活跃...（200字无换行）
✅ 正确：
关键数据：
- **6307 stars** - 社区关注度高
- **2025-11-14** 最新提交 - 仍在活跃开发

**3. 视觉引导符号** - 使用✅ 优势、⚠️ 需要注意、💡 建议
✅ 优势：
- 代码和评估脚本在GitHub公开
- 数据集可通过Hugging Face下载

⚠️ 需要注意：
- 评估脚本链接不明确
- 数据集规模缺少详细说明

**4. 关键信息加粗** - 数字、结论、风险点
- **6307 stars** 不是 6307 stars
- **MIT License** 不是 MIT License
- **P0优先级** 不是 P0优先级

**5. 批判性思维** - 必须指出问题和风险
不要只说"该项目表现出色"，还要说"需要注意XXX"

**6. 避免过于正式** - 不要用"该候选项"、"该项目"
✅ 用"项目"、直接说事
❌ 不要"该候选项在GitHub上拥有..."
✅ 改成"GitHub上有6307 stars..."

**7. 结构化输出** - 使用markdown列表、分段
关键数据：
- 第1点
- 第2点

对MGX的价值：
简短总结

⭐ 推荐纳入/不推荐

**8. 字符数计算** - 不包括markdown符号（**、-、空格）
"**6307 stars**" 计算为11字符（"6307 stars"）

**9. 完整示例对比**

❌ AI味重（禁止这样写）：
"【活跃度】该候选项在GitHub上拥有6307个stars，显示出显著的社区关注度。最近的提交日期为2025年11月14日，表明项目仍在活跃维护中。项目的PR和Issue讨论活跃，当前有多个open issues正在被处理，显示出良好的社区参与度和治理模式。这种活跃度对于MGX来说是一个积极的信号，因为持续的维护意味着更少的技术债务和更好的兼容性。因此，MGX可以考虑采纳该项目，以利用其活跃的社区支持和持续的更新。"

✅ 人性化（必须这样写）：
"**活跃度 10/10** - 社区热度高，维护积极

关键数据：
- **6307 stars** - 社区关注度高
- **2025-11-14** 最新提交 - 仍在活跃开发
- Issue/PR 讨论活跃 - 问题响应快

对MGX的价值：
持续维护意味着更少的技术债和更好的兼容性。遇到问题时有社区支持，不是孤军奋战。

⭐ 推荐纳入：社区支持强，长期可维护性好。"

```

**修改说明**:
- 新增完整的写作风格指南（约100行）
- 提供详细的Before vs After示例
- 强调9个写作规则，确保LLM遵守

---

### Step 2: 修改5个维度推理示例

#### 2.1 修改 activity_reasoning 示例（第114行）

**文件路径**: `src/scorer/llm_scorer.py`

**当前代码**（第114行）:
```python
- **字符计数示例**："该候选项来自GitHub，拥有1200+ stars，说明有一定的社区关注度。最近30天内有5次提交，表明项目仍在活跃维护中。PR讨论较活跃，有15个open issues正在被处理，社区参与度良好。这种活跃度适合MGX采纳，因为持续维护意味着更少技术债和更好的兼容性。"（≈180字符）
```

**修改后**:
```python
- **字符计数示例**（markdown格式，字符数不包括**、-、空格）：
"""
**活跃度 8/10** - 社区活跃，维护良好

关键数据：
- **1200+ stars** - 社区关注度中等
- **5次提交/最近30天** - 仍在维护
- **15个open issues** - 社区参与积极

对MGX的价值：
持续维护意味着更少技术债和更好的兼容性。

⭐ 推荐纳入
"""
（计算字符数：去除markdown符号后约180字符）
```

#### 2.2 修改 reproducibility_reasoning 示例（第130行）

**当前代码**（第130行）:
```python
- **字符计数示例**："代码与评估脚本均在GitHub公开，并附有Dockerfile方便还原环境。数据集通过Hugging Face下载，无需审批但体积达80GB，需要具备A100×2的算力才能重现作者报告的成绩。README列出逐步命令，但缺乏自动化验证脚本，MGX需要额外编写任务编排器来驱动评估。"（≈190字符）
```

**修改后**:
```python
- **字符计数示例**（markdown格式）：
"""
**可复现性 8/10** - 代码公开，文档完善

✅ 优势：
- 代码+评估脚本在GitHub公开
- Dockerfile还原环境方便
- 数据集通过Hugging Face下载

⚠️ 需要注意：
- 数据集体积80GB
- 需要A100×2算力
- 缺少自动化验证脚本

对MGX的影响：
需要编写任务编排器来驱动评估。
"""
（≈190字符）
```

#### 2.3 修改 license_reasoning 示例（第146行）

**当前代码**（第146行）:
```python
- **字符计数示例**："仓库根目录提供MIT License，GitHub API同样标记为MIT，意味着可自由商用、修改与再分发且无传染条款。对于MGX而言，可直接集成其数据与脚本，不会强制开源自研组件。唯一需要注意的是依赖的第三方模型需单独确认许可。总体来看，该许可完全满足MGX的商业与合规需求。"（≈185字符）
```

**修改后**:
```python
- **字符计数示例**（markdown格式）：
"""
**许可合规 10/10** - MIT协议，商业友好

关键信息：
- **MIT License** - 可自由使用、修改、再分发
- 无传染性条款 - 不强制开源自研组件
- 商业化友好 - 无额外限制

⚠️ 唯一需要注意：
依赖的第三方库/模型许可证需单独确认。

对MGX的价值：
可以安全集成，不用担心许可证风险。
"""
（≈185字符）
```

#### 2.4 修改 novelty_reasoning 示例（第162行）

**当前代码**（第162行）:
```python
- **字符计数示例**："该Benchmark发布于2025年3月，引入"多Agent分工+代码审阅"任务，相比HumanEval只测单轮生成，它额外考察沟通准确率和审阅反馈质量。指标方面新增交互轮次成功率，弥补MGX在协作编码评测上的空白。即便延续经典Pass@k，也通过高难度多文件项目提高区分度。"（≈190字符）
```

**修改后**:
```python
- **字符计数示例**（markdown格式）：
"""
**新颖性 9/10** - 填补MGX协作编码空白

创新点：
- 多Agent分工+代码审阅（vs HumanEval单轮生成）
- 交互轮次成功率指标（新增）
- 高难度多文件项目（区分度高）

对MGX的价值：
弥补协作编码评测空白，提供沟通准确率和审阅反馈质量的新维度。

⭐ 推荐纳入
"""
（≈190字符）
```

#### 2.5 修改 relevance_reasoning 示例（第178行）

**当前代码**（第178行）:
```python
- **字符计数示例**："任务要求多Agent协作完成Web端漏洞修复，核心流程包括阅读源码、生成补丁、运行E2E测试，明显属于P0级Coding+WebDev场景。MGX可直接把该基准作为Vibe Coding协作任务，衡量Agent在代码检索、工具链调用及回归测试的闭环能力，适配成本仅需编写浏览器控制工具。"（≈200字符）
```

**修改后**:
```python
- **字符计数示例**（markdown格式）：
"""
**MGX适配度 9/10** - 核心场景高度匹配

任务覆盖：
- ✅ 多Agent协作（Web端漏洞修复）
- ✅ 代码检索+补丁生成+E2E测试
- ✅ P0级Coding+WebDev场景

适配成本评估：
仅需编写浏览器控制工具，成本低。

💡 建议：
优先纳入，用于评估Vibe Coding协作闭环能力。
"""
（≈200字符）
```

---

### Step 3: 修改 overall_reasoning 要求（第269-274行）

**文件路径**: `src/scorer/llm_scorer.py`

**当前代码**（第269-274行）:
```python
【综合推理 overall_reasoning】（≥50字符）
基于5维评分，给出总体推荐意见：
- 如果 总分≥6.5 且 relevance_score≥7：强烈推荐纳入MGX
- 如果 总分≥6.0 且 relevance_score≥5：推荐纳入，但需优先级排序
- 如果 总分<6.0 或 relevance_score<5：不推荐纳入
- 说明主要优势和主要风险
```

**修改后**:
```python
【综合推理 overall_reasoning】（≥200字符，markdown格式）

必须包含以下4个部分：

**1. 核心结论**（1-2句话）
- 总体推荐意见：强烈推荐/推荐/不推荐
- 总分和关键维度分数

**2. 主要优势**（2-3点）
- 用✅标记
- 突出最关键的优势

**3. 需要注意**（1-2点）
- 用⚠️标记
- 指出风险或限制

**4. 决策建议**（1-2句话）
- 用💡标记
- 给出明确的行动建议

**推荐逻辑**:
- 如果 总分≥6.5 且 relevance_score≥7：⭐ **强烈推荐纳入MGX**
- 如果 总分≥6.0 且 relevance_score≥5：✅ **推荐纳入**，但需优先级排序
- 如果 总分<6.0 或 relevance_score<5：❌ **不推荐纳入**

**示例**（markdown格式）:
"""
## 💡 核心结论

这个项目在GUI自动化和多Agent协作领域表现出色，与MGX的核心场景高度契合。总分9.0/10，MGX适配度9/10。

⭐ **强烈推荐纳入MGX**

---

✅ **主要优势**:
- 社区活跃（6307 stars，近期维护积极）
- MIT协议商业友好，无许可证风险
- 填补MGX在GUI自动化的空白

⚠️ **需要注意**:
- 适配成本中等，需要工程改造
- 文档详细度有待提升

💡 **决策建议**:
优先纳入，用于评估MGX在GUI自动化和多Agent协作方面的能力。
"""
（≈230字符）
```

**修改说明**:
- 将最小字符数从50提升到200
- 要求4部分结构化输出
- 提供完整的markdown示例
- 强调使用✅/⚠️/💡符号

---

### Step 4: 修改后端专项推理示例

#### 4.1 修改 backend_mgx_reasoning 示例（第197行）

**文件路径**: `src/scorer/llm_scorer.py`

**当前代码**（第197行）:
```python
- **字符计数示例**："该后端Benchmark专注评测Web框架在高并发下的吞吐量、P95延迟与资源占用，并提供REST与GraphQL两种模式。MGX可用它来测试自动生成的FastAPI/Express服务，在真实压测脚本下比较代理编写代码与人工基线的性能差异。场景涵盖TLS终止和数据库交互，能够为MGX的后端Agent提供可量化的优化目标。"（≈240字符）
```

**修改后**:
```python
- **字符计数示例**（markdown格式）：
"""
**后端MGX相关性 9/10** - 直接评测后端生成能力

评测维度：
- 吞吐量、P95延迟、资源占用
- REST与GraphQL两种模式
- TLS终止+数据库交互

对MGX的价值：
可用于测试自动生成的FastAPI/Express服务，在真实压测下比较代理代码与人工基线的性能差异。

⭐ 推荐纳入
"""
（≈240字符）
```

#### 4.2 修改 backend_engineering_reasoning 示例（第213行）

**当前代码**（第213行）:
```python
- **字符计数示例**："测试框架提供Docker Compose与k6脚本，可在30分钟内复现压测，并导出Prometheus指标，工程成熟度较高。虽然负载模型集中在API读写，对复杂分布式事务覆盖不足，但足以指导MGX评估自动生成的后端应用在CPU/内存曲线上的表现。建议MGX采纳该基准作为性能回归测试，同时补充数据库写压场景。"（≈250字符）
```

**修改后**:
```python
- **字符计数示例**（markdown格式）：
"""
**工程实践价值 8/10** - 工程化程度高

✅ 优势：
- Docker Compose+k6脚本
- 30分钟内可复现压测
- Prometheus指标导出

⚠️ 局限：
- 负载模型集中API读写
- 对复杂分布式事务覆盖不足

💡 建议：
采纳作为性能回归测试，同时补充数据库写压场景。
"""
（≈250字符）
```

---

### Step 5: 修改推理字数验证（第276-280行）

**文件路径**: `src/scorer/llm_scorer.py`

**当前代码**（第276-280行）:
```python
【推理字数验证】
- activity_reasoning + reproducibility_reasoning + license_reasoning + novelty_reasoning + relevance_reasoning ≥ 750字符
- backend_mgx_reasoning + backend_engineering_reasoning ≥ 400字符（仅后端）
- overall_reasoning ≥ 50字符
- **总推理字数必须 ≥1200字符（非后端Benchmark至少800字符）**
```

**修改后**:
```python
【推理字数验证】
- activity_reasoning + reproducibility_reasoning + license_reasoning + novelty_reasoning + relevance_reasoning ≥ 750字符
- backend_mgx_reasoning + backend_engineering_reasoning ≥ 400字符（仅后端）
- overall_reasoning ≥ 200字符（从50字符提升）
- **总推理字数必须 ≥1400字符（非后端Benchmark至少1000字符）**
- **字符数计算方法**：去除markdown符号（**、-、#、空格）后的纯文本字符数
```

**修改说明**:
- overall_reasoning从50字符提升到200字符
- 总推理字数从1200字符提升到1400字符（非后端从800提升到1000）
- 明确字符数计算方法

---

## ✅ 测试验证

### 验证方法1: 手动检查输出格式（推荐）

**执行命令**:
```bash
.venv/bin/python -m src.main
```

**检查日志**:
```bash
# 查看最新日志中的评分依据
tail -500 logs/benchscope.log | grep -A 50 "activity_reasoning"
```

**验收点**:
- ✅ 是否使用`**活跃度 X/10**`格式（无【】）
- ✅ 是否有分段和列表
- ✅ 是否有✅/⚠️/💡符号
- ✅ 关键数据是否加粗（**数字**）
- ✅ 综合推理是否≥200字符
- ✅ 是否有"需要注意"部分（批判性思维）

---

### 验证方法2: 随机抽查飞书表格

**操作步骤**:
1. 打开飞书多维表格
2. 随机选择10条最新记录
3. 查看"评分依据"字段

**验收点**:
- ✅ markdown格式是否正确渲染
- ✅ 加粗、列表是否显示正确
- ✅ 阅读体验是否提升（快速抓住重点）
- ✅ 是否有批判性思维（指出问题）

---

### 验证方法3: 字符数验证脚本

**创建测试脚本**: `scripts/test_humanize_reasoning.py`

```python
"""测试人性化推理输出格式"""

from __future__ import annotations

import re
import sys
from pathlib import Path

sys.path.append(str(Path(__file__).resolve().parent.parent))


def count_chars_without_markdown(text: str) -> int:
    """计算去除markdown符号后的字符数"""
    # 去除**、-、#、多余空格
    cleaned = re.sub(r'\*\*|\-|\#|\s{2,}', '', text)
    # 去除换行符
    cleaned = cleaned.replace('\n', '')
    return len(cleaned.strip())


def test_reasoning_format():
    """验证推理输出格式"""
    print("=" * 60)
    print("测试：人性化推理输出格式验证")
    print("=" * 60)

    # 模拟LLM输出
    sample_reasoning = """
**活跃度 10/10** - 社区热度高，维护积极

关键数据：
- **6307 stars** - 社区关注度高
- **2025-11-14** 最新提交 - 仍在活跃开发
- Issue/PR 讨论活跃 - 问题响应快

对MGX的价值：
持续维护意味着更少的技术债和更好的兼容性。遇到问题时有社区支持，不是孤军奋战。

⭐ 推荐纳入：社区支持强，长期可维护性好。
"""

    # 验证1: 无【】标记
    assert "【" not in sample_reasoning, "❌ 仍有【】标记"
    print("✅ 无【】标记")

    # 验证2: 有markdown格式
    assert "**" in sample_reasoning, "❌ 缺少markdown加粗"
    print("✅ 有markdown加粗")

    # 验证3: 有列表
    assert "-" in sample_reasoning, "❌ 缺少列表"
    print("✅ 有列表格式")

    # 验证4: 有视觉引导符号
    assert "⭐" in sample_reasoning or "✅" in sample_reasoning or "⚠️" in sample_reasoning, "❌ 缺少视觉引导符号"
    print("✅ 有视觉引导符号")

    # 验证5: 字符数≥150
    char_count = count_chars_without_markdown(sample_reasoning)
    print(f"\n字符数（去除markdown符号）: {char_count}")
    assert char_count >= 150, f"❌ 字符数不足: {char_count} < 150"
    print("✅ 字符数达标")

    # 验证6: 有分段
    lines = [line.strip() for line in sample_reasoning.strip().split('\n') if line.strip()]
    assert len(lines) >= 5, "❌ 分段不足"
    print(f"✅ 分段良好（{len(lines)}行）")

    print("\n" + "=" * 60)
    print("✅ 所有测试通过！")
    print("=" * 60)


def test_overall_reasoning_format():
    """验证综合推理格式"""
    print("\n" + "=" * 60)
    print("测试：综合推理格式验证")
    print("=" * 60)

    sample_overall = """
## 💡 核心结论

这个项目在GUI自动化和多Agent协作领域表现出色，与MGX的核心场景高度契合。总分9.0/10，MGX适配度9/10。

⭐ **强烈推荐纳入MGX**

---

✅ **主要优势**:
- 社区活跃（6307 stars，近期维护积极）
- MIT协议商业友好，无许可证风险
- 填补MGX在GUI自动化的空白

⚠️ **需要注意**:
- 适配成本中等，需要工程改造
- 文档详细度有待提升

💡 **决策建议**:
优先纳入，用于评估MGX在GUI自动化和多Agent协作方面的能力。
"""

    # 验证1: 有4个部分
    assert "核心结论" in sample_overall, "❌ 缺少核心结论"
    assert "主要优势" in sample_overall, "❌ 缺少主要优势"
    assert "需要注意" in sample_overall, "❌ 缺少需要注意"
    assert "决策建议" in sample_overall, "❌ 缺少决策建议"
    print("✅ 4个部分完整")

    # 验证2: 有批判性思维
    assert "⚠️" in sample_overall, "❌ 缺少批判性思维（需要注意）"
    print("✅ 有批判性思维")

    # 验证3: 字符数≥200
    char_count = count_chars_without_markdown(sample_overall)
    print(f"\n字符数（去除markdown符号）: {char_count}")
    assert char_count >= 200, f"❌ 字符数不足: {char_count} < 200"
    print("✅ 字符数达标")

    print("\n" + "=" * 60)
    print("✅ 所有测试通过！")
    print("=" * 60)


if __name__ == "__main__":
    test_reasoning_format()
    test_overall_reasoning_format()
```

**执行命令**:
```bash
.venv/bin/python scripts/test_humanize_reasoning.py
```

**预期输出**:
```
============================================================
测试：人性化推理输出格式验证
============================================================
✅ 无【】标记
✅ 有markdown加粗
✅ 有列表格式
✅ 有视觉引导符号

字符数（去除markdown符号）: 185
✅ 字符数达标
✅ 分段良好（12行）

============================================================
✅ 所有测试通过！
============================================================

============================================================
测试：综合推理格式验证
============================================================
✅ 4个部分完整
✅ 有批判性思维

字符数（去除markdown符号）: 235
✅ 字符数达标

============================================================
✅ 所有测试通过！
============================================================
```

---

## 📝 检查清单

### 代码质量检查
- [ ] PEP8合规（使用`black .`格式化）
- [ ] 代码无语法错误（使用`ruff check .`检查）
- [ ] Prompt字符串正确闭合（无语法错误）
- [ ] 示例markdown格式正确

### 功能验证检查
- [ ] 完整流程测试通过（`python -m src.main`）
- [ ] 输出无【】标记
- [ ] 输出有markdown格式（加粗、列表）
- [ ] 输出有视觉引导符号（✅/⚠️/💡）
- [ ] 综合推理≥200字符
- [ ] 飞书表格markdown渲染正确
- [ ] 随机抽查10条评分依据，阅读体验提升

### Linus哲学验证
- [ ] **Is this a real problem?** ✅ 是真实问题（用户明确反馈AI味重）
- [ ] **Is there a simpler way?** ✅ 最简方案（只修改Prompt，不改代码逻辑）
- [ ] **What will this break?** ✅ 零破坏（向后兼容，只是格式变化）

---

## 🚨 风险应对

### 风险1: LLM不遵守格式要求

**监控指标**: 检查输出是否仍有【】

**应对措施**:
- 在Prompt中多次强调（3次）
- 提供详细Before vs After示例
- 如果不遵守，Self-Healing重试1次

### 风险2: 字符数计算错误

**监控指标**: Pydantic验证失败

**应对措施**:
- 明确说明字符数不包括markdown符号
- 在验证逻辑中去除markdown符号再计数
- 提供字符数计算示例

### 风险3: markdown渲染异常

**监控指标**: 飞书表格显示异常

**应对措施**:
- 测试飞书表格markdown渲染
- 如果异常，调整markdown语法
- 备用方案：使用纯文本（去除markdown）

---

## 🎉 成功标准

### 量化指标
| 指标 | 当前 | 目标 | 验收标准 |
|------|------|---------|----------|
| **【】使用率** | 100% | 0% | 输出中无【】 |
| **关键信息加粗率** | 0% | 80% | 数字/结论加粗 |
| **视觉引导符号** | 0% | 100% | ✅/⚠️/💡出现 |
| **综合推理长度** | 50-100字符 | 200-300字符 | ≥200字符 |

### 定性指标
- 阅读体验提升（快速抓住重点）
- 批判性思维（指出问题）
- 决策支持度提高（结构清晰）

---

## 📌 注意事项

1. **Prompt字符串闭合**: 确保所有三引号正确闭合
2. **示例格式正确**: markdown示例必须可渲染
3. **字符数计算**: 不包括markdown符号
4. **向后兼容**: 不修改JSON Schema，只是内容格式变化
5. **测试覆盖**: 随机抽查≥10条评分依据

---

## 📚 参考文档

- **PRD文档**: `.claude/specs/benchmark-intelligence-agent/PHASE-HUMANIZE-REASONING-PRD.md`
- **用户反馈**: 2025-11-21原始需求
- **当前Prompt**: `src/scorer/llm_scorer.py`第90-389行
- **全局写作规范**: `~/.claude/CLAUDE.md`的"Report Generation Rules"

---

**开发完成后请执行以下操作**：

1. ✅ 运行完整流程：`.venv/bin/python -m src.main`
2. ✅ 运行测试脚本：`.venv/bin/python scripts/test_humanize_reasoning.py`
3. ✅ 检查日志：验证输出格式
4. ✅ 验证飞书表格：检查markdown渲染
5. ✅ 随机抽查10条评分依据
6. ✅ 通知Claude Code：提供Before vs After对比截图

**Claude Code将执行最终验收测试！**
